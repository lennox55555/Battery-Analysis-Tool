{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "533cbe4a-fef8-4dba-bf89-7a68e3618473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/lib/python3.12/site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: plotly in /opt/anaconda3/lib/python3.12/site-packages (5.22.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from plotly) (8.2.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from plotly) (23.2)\n"
     ]
    }
   ],
   "source": [
    "# install pandas for data manipulation and analysis\n",
    "!pip install pandas\n",
    "# install numpy for numerical operations\n",
    "!pip install numpy\n",
    "# install matplotlib for static data visualization\n",
    "!pip install matplotlib\n",
    "# install seaborn for advanced statistical visualizations\n",
    "!pip install seaborn\n",
    "# install plotly for interactive visualizations\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b7f3110-d5e4-40ff-892c-96e69cd5de6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of all CSV files in the folder...\n",
      "Processing file 1 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 7.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 7.csv. 85 anomalies detected. 3 plots generated.\n",
      "Processing file 2 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 6.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 6.csv. 1 anomalies detected. 1 plots generated.\n",
      "Processing file 3 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 4.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 4.csv. 0 anomalies detected. 0 plots generated.\n",
      "Processing file 4 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 5.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 5.csv. 302 anomalies detected. 5 plots generated.\n",
      "Processing file 5 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 1.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 1.csv. 0 anomalies detected. 0 plots generated.\n",
      "Processing file 6 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 2.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 2.csv. 0 anomalies detected. 0 plots generated.\n",
      "Processing file 7 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 3.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 3.csv. 2 anomalies detected. 2 plots generated.\n",
      "Processing file 8 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 11.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 11.csv. 39 anomalies detected. 4 plots generated.\n",
      "Processing file 9 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 10.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 10.csv. 1 anomalies detected. 1 plots generated.\n",
      "Processing file 10 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 12.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 12.csv. 0 anomalies detected. 0 plots generated.\n",
      "Processing file 11 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 8.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 8.csv. 4 anomalies detected. 1 plots generated.\n",
      "Processing file 12 of 12: BMS2 - Data Export-data-as-joinbyfield-2024-04-26 9.csv\n",
      "Finished processing BMS2 - Data Export-data-as-joinbyfield-2024-04-26 9.csv. 79 anomalies detected. 5 plots generated.\n",
      "\n",
      "Combined anomaly and downtime summary exported to: /Users/lennox/Desktop/Lennox/output/combined_anomaly_summary.csv\n",
      "Interactive plots saved to: /Users/lennox/Desktop/Lennox/output/anomaly_plots.html\n",
      "Total number of plots generated: 22\n",
      "\n",
      "Analysis complete. Summary of detected anomalies and downtimes:\n",
      "                                Rack              Type  \\\n",
      "0                                All  Service Downtime   \n",
      "1  bms_2_rack_1_min_cell_voltage (V)       Malfunction   \n",
      "2  bms_2_rack_2_min_cell_voltage (V)       Malfunction   \n",
      "\n",
      "   Number of anomalies detected  First anomaly time   Last anomaly time  \\\n",
      "0                             4 2024-02-19 12:00:00 2024-04-12 12:00:00   \n",
      "1                           470 2024-03-02 02:32:44 2024-04-06 07:26:21   \n",
      "2                            39 2024-04-18 18:54:00 2024-04-20 12:31:49   \n",
      "\n",
      "   Total duration  Max Rate of Change  \n",
      "0 0 days 15:29:36                 NaN  \n",
      "1 1 days 14:25:17             0.00168  \n",
      "2 0 days 16:52:31             0.00189  \n",
      "\n",
      "All plots have been generated and saved in the output folder.\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries for file operations, data analysis, visualization, and date manipulation\n",
    "import os  # used for interacting with the operating system, such as file path management\n",
    "import pandas as pd  # used for data manipulation and analysis with DataFrames\n",
    "import numpy as np  # provides support for numerical operations and array handling\n",
    "import matplotlib.pyplot as plt  # used for creating static plots and charts\n",
    "import seaborn as sns  # used for creating attractive statistical visualizations\n",
    "from datetime import timedelta  # used for time calculations, such as adding or subtracting time\n",
    "import plotly.graph_objects as go  # used for creating interactive visualizations with Plotly\n",
    "from plotly.subplots import make_subplots  # used for creating subplots with Plotly\n",
    "\n",
    "def load_and_clean_battery_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads and cleans battery data from a CSV file.\n",
    "\n",
    "    Params:\n",
    "    file_path (str): Path to the CSV file containing battery data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Cleaned DataFrame containing battery data with 'Time' as index.\n",
    "    \"\"\"\n",
    "    # loads the raw data from CSV; skips first row and uses no predefined header to accommodate custom column names\n",
    "    df = pd.read_csv(file_path, skiprows=1, header=None, low_memory=False)\n",
    "\n",
    "    # sets column names using the first row of data; assigns meaningful labels to each column for easier reference\n",
    "    df.columns = df.iloc[0]\n",
    "\n",
    "    # removes the row used for column names; resets the index to start from 0 for consistency\n",
    "    df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    # converts 'Time' column to datetime format; ensures proper handling of time-based operations in analysis\n",
    "    df['Time'] = pd.to_datetime(df['Time'])\n",
    "\n",
    "    # sets 'Time' as the index of the DataFrame; allows for time-based data access and manipulation\n",
    "    df.set_index('Time', inplace=True)\n",
    "\n",
    "    # identifies columns that should be numeric; finds columns currently stored as strings for conversion\n",
    "    numeric_columns = df.columns[df.dtypes == 'object']\n",
    "\n",
    "    # converts identified columns to numeric type; replaces non-numeric values with NaN to handle errors gracefully\n",
    "    for col in numeric_columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # returns the cleaned DataFrame for further analysis or visualization\n",
    "    return df\n",
    "\n",
    "def calculate_dynamic_thresholds(window_data, voltage_columns, deviation_factor=0.05):\n",
    "    \"\"\"\n",
    "    Calculates dynamic thresholds for voltage data.\n",
    "\n",
    "    Params:\n",
    "    window_data (pd.DataFrame): DataFrame containing voltage data for a specific time window.\n",
    "    voltage_columns (list): List of column names containing voltage data.\n",
    "    deviation_factor (float): Factor to determine threshold range.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with average, upper_threshold, and lower_threshold columns.\n",
    "    \"\"\"\n",
    "    # nested function to calculate the average of top 5 values in a row, excluding the lowest to minimize outlier impact\n",
    "    def avg_top_5(row):\n",
    "        # sorts values in ascending order to exclude the lowest outliers for a more robust mean calculation\n",
    "        sorted_values = np.sort(row.values)\n",
    "        # calculates mean of the top 5 values to smooth data and reduce influence of extreme values\n",
    "        return np.mean(sorted_values[1:6])\n",
    "\n",
    "    # computes average voltage across specified columns using the top 5 method for each row to derive a smooth baseline\n",
    "    avg_voltage = window_data[voltage_columns].apply(avg_top_5, axis=1)\n",
    "\n",
    "    # calculates upper voltage limit based on the deviation factor; defines the upper bound of acceptable voltage range\n",
    "    upper_threshold = avg_voltage * (1 + deviation_factor)\n",
    "\n",
    "    # calculates lower voltage limit based on the deviation factor; defines the lower bound of acceptable voltage range\n",
    "    lower_threshold = avg_voltage * (1 - deviation_factor)\n",
    "\n",
    "    # returns a DataFrame containing the calculated average and dynamic thresholds for each time point in the window\n",
    "    return pd.DataFrame({\n",
    "        'average': avg_voltage,\n",
    "        'upper_threshold': upper_threshold,\n",
    "        'lower_threshold': lower_threshold\n",
    "    })\n",
    "\n",
    "def calculate_rate_of_change(series, window='10min'):\n",
    "    \"\"\"\n",
    "    Calculates the rate of change for a time series.\n",
    "\n",
    "    Params:\n",
    "    series (pd.Series): Time series data to calculate rate of change.\n",
    "    window (str): Time window for rolling calculation.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: Rate of change in volts per hour.\n",
    "    \"\"\"\n",
    "    # computes the change in voltage over a rolling window; smooths short-term fluctuations in the data\n",
    "    # multiplies by 6 to convert the change from per minute to per hour for consistency in measurement units\n",
    "    return series.diff().rolling(window=window).mean() * 6\n",
    "\n",
    "def plot_min_voltages_with_dynamic_thresholds(df, file_name, window_hours=12, deviation_factor=0.05):\n",
    "    \"\"\"\n",
    "    Plots minimum cell voltages and identifies anomalies using dynamic thresholds.\n",
    "\n",
    "    Params:\n",
    "    df (pd.DataFrame): DataFrame containing battery voltage data.\n",
    "    file_name (str): Name of the file being processed (used for plot titles).\n",
    "    window_hours (int): Number of hours for each analysis window.\n",
    "    deviation_factor (float): Factor to determine threshold range.\n",
    "\n",
    "    Returns:\n",
    "    tuple: List of detected anomalies and list of Plotly figure objects for windows with anomalies.\n",
    "    \"\"\"\n",
    "    # identifies columns containing minimum cell voltage data for further analysis\n",
    "    min_voltage_columns = [col for col in df.columns if 'min_cell_voltage' in col]\n",
    "    \n",
    "    # calculates total duration of data and determines the number of windows based on window_hours\n",
    "    total_duration = df.index[-1] - df.index[0]\n",
    "    num_windows = max(1, int(total_duration.total_seconds() / (window_hours * 3600)))\n",
    "    \n",
    "    # initializes lists to store anomalies and plots for each window of analysis\n",
    "    anomalies = []\n",
    "    anomaly_plots = []\n",
    "    \n",
    "    # iterates through each time window to analyze voltage data\n",
    "    for window in range(num_windows):\n",
    "        # determines the start and end time of the current analysis window\n",
    "        start_time = df.index[0] + timedelta(hours=window * window_hours)\n",
    "        end_time = min(start_time + timedelta(hours=window_hours), df.index[-1])\n",
    "        \n",
    "        # selects data within the current time window for analysis\n",
    "        window_data = df.loc[start_time:end_time]\n",
    "        \n",
    "        # skips processing if the window data is empty to avoid errors\n",
    "        if window_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # calculates dynamic thresholds for voltage columns within the current window\n",
    "        thresholds = calculate_dynamic_thresholds(window_data, min_voltage_columns, deviation_factor)\n",
    "        \n",
    "        # calculates the rate of change for each voltage column to identify steep drops or rises\n",
    "        rates_of_change = pd.DataFrame({col: calculate_rate_of_change(window_data[col]) for col in min_voltage_columns})\n",
    "        \n",
    "        # identifies periods with a high rate of change, which might indicate a malfunction\n",
    "        high_slope_mask = (rates_of_change.abs() > 0.0019).any(axis=1)\n",
    "        \n",
    "        # expands the high slope period to account for potential pre- and post-failure behavior within 15 minutes\n",
    "        expanded_high_slope_mask = pd.Series(False, index=window_data.index)\n",
    "        for idx in high_slope_mask[high_slope_mask].index:\n",
    "            expanded_high_slope_mask |= (window_data.index >= idx - timedelta(minutes=15)) & (window_data.index <= idx + timedelta(minutes=15))\n",
    "        \n",
    "        # identifies voltage readings within a certain low voltage range, which might indicate a service downtime\n",
    "        low_voltage_mask = (window_data[min_voltage_columns] >= -0.25) & (window_data[min_voltage_columns] <= 0.25)\n",
    "        # checks for any occurrence of low voltage across all columns within the time window\n",
    "        any_low_voltage = low_voltage_mask.any(axis=1)\n",
    "        \n",
    "        # expands identified low voltage periods to include a buffer of 2 hours before and after\n",
    "        service_downtime_mask = pd.Series(False, index=window_data.index)\n",
    "        for idx in window_data[any_low_voltage].index:\n",
    "            service_downtime_mask |= (window_data.index >= idx - timedelta(hours=2)) & (window_data.index <= idx + timedelta(hours=2))\n",
    "        \n",
    "        # initializes a list to store anomalies for the current window of data\n",
    "        window_anomalies = []\n",
    "        \n",
    "        # iterates through each voltage column to identify anomalies based on dynamic thresholds and masks\n",
    "        for column in min_voltage_columns:\n",
    "            # identifies points where the voltage is outside the dynamic threshold range\n",
    "            anomaly_mask = (window_data[column] < thresholds['lower_threshold']) | (window_data[column] > thresholds['upper_threshold'])\n",
    "            # filters out anomalies that coincide with service downtimes or high slope periods\n",
    "            true_anomaly_mask = anomaly_mask & ~service_downtime_mask & ~expanded_high_slope_mask\n",
    "            \n",
    "            # groups contiguous time periods of anomalies for further processing\n",
    "            anomaly_periods = window_data[true_anomaly_mask].groupby((~true_anomaly_mask).cumsum())\n",
    "            for _, period in anomaly_periods:\n",
    "                # ensures the period is not empty before processing it as an anomaly\n",
    "                if len(period) > 0:\n",
    "                    # calculates the maximum rate of change during the anomaly period to assess severity\n",
    "                    max_roc = rates_of_change.loc[period.index, column].abs().max()\n",
    "                    # filters out anomalies with negligible rate of change to focus on significant malfunctions\n",
    "                    if max_roc > 0.00005:\n",
    "                        # stores details of the anomaly for summary and further analysis\n",
    "                        anomaly = {\n",
    "                            'Rack': column,\n",
    "                            'Start_Time': period.index[0],\n",
    "                            'End_Time': period.index[-1],\n",
    "                            'Duration': period.index[-1] - period.index[0],\n",
    "                            'Max_RoC': max_roc,\n",
    "                            'Type': 'Malfunction'\n",
    "                        }\n",
    "                        # adds the detected anomaly to the list for the current window\n",
    "                        anomalies.append(anomaly)\n",
    "                        window_anomalies.append(anomaly)\n",
    "        \n",
    "        # identifies contiguous periods of service downtimes for reporting\n",
    "        downtime_periods = window_data[service_downtime_mask].groupby((~service_downtime_mask).cumsum())\n",
    "        for _, period in downtime_periods:\n",
    "            if len(period) > 0:\n",
    "                downtime = {\n",
    "                    'Rack': 'All',\n",
    "                    'Start_Time': period.index[0],\n",
    "                    'End_Time': period.index[-1],\n",
    "                    'Duration': period.index[-1] - period.index[0],\n",
    "                    'Max_RoC': None,\n",
    "                    'Type': 'Service Downtime'\n",
    "                }\n",
    "                anomalies.append(downtime)\n",
    "                window_anomalies.append(downtime)\n",
    "        \n",
    "        # generates a plot for the current window if any anomalies were detected\n",
    "        if window_anomalies:\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # adds voltage data traces for each rack to the plot for visual analysis\n",
    "            for column in min_voltage_columns:\n",
    "                fig.add_trace(go.Scatter(x=window_data.index, y=window_data[column], mode='lines', name=column))\n",
    "            \n",
    "            # tracks whether legend entries for malfunctions and downtimes have been added to avoid duplicates\n",
    "            malfunction_added = False\n",
    "            downtime_added = False\n",
    "            \n",
    "            # iterates through each detected anomaly to visualize them on the plot\n",
    "            for anomaly in window_anomalies:\n",
    "                if anomaly['Type'] == 'Malfunction':\n",
    "                    # adds a point marker for malfunction anomalies on the plot\n",
    "                    showlegend = not malfunction_added\n",
    "                    malfunction_added = True\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=[anomaly['Start_Time'], anomaly['End_Time']],\n",
    "                        y=[window_data.loc[anomaly['Start_Time'], anomaly['Rack']], window_data.loc[anomaly['End_Time'], anomaly['Rack']]],\n",
    "                        mode='markers',\n",
    "                        marker=dict(color='red', size=10),\n",
    "                        name='Malfunction',\n",
    "                        showlegend=showlegend\n",
    "                    ))\n",
    "                elif anomaly['Type'] == 'Service Downtime':\n",
    "                    # adds shaded regions for service downtimes on the plot for easier interpretation\n",
    "                    fig.add_vrect(\n",
    "                        x0=anomaly['Start_Time'], x1=anomaly['End_Time'],\n",
    "                        fillcolor=\"blue\", opacity=0.3,\n",
    "                        layer=\"below\", line_width=0,\n",
    "                        name='Service Downtime',\n",
    "                        showlegend=not downtime_added\n",
    "                    )\n",
    "                    downtime_added = True\n",
    "            \n",
    "            # adds traces for upper and lower thresholds as dashed lines on the plot\n",
    "            fig.add_trace(go.Scatter(x=thresholds.index, y=thresholds['upper_threshold'], mode='lines', name='Upper Threshold', line=dict(color='red', dash='dash')))\n",
    "            fig.add_trace(go.Scatter(x=thresholds.index, y=thresholds['lower_threshold'], mode='lines', name='Lower Threshold', line=dict(color='red', dash='dash')))\n",
    "            fig.add_trace(go.Scatter(x=thresholds.index, y=thresholds['average'], mode='lines', name='Average of Top 5', line=dict(color='green', dash='dot')))\n",
    "            \n",
    "            # updates the plot layout for better readability and interpretation\n",
    "            fig.update_layout(\n",
    "                title=f\"{file_name}: Minimum Cell Voltages by Rack with Dynamic Thresholds ({start_time} to {end_time})\",\n",
    "                xaxis_title=\"Time\",\n",
    "                yaxis_title=\"Voltage (V)\",\n",
    "                legend_title=\"Rack\",\n",
    "                hovermode=\"x unified\"\n",
    "            )\n",
    "            \n",
    "            # adds the plot to the list of plots for later export and visualization\n",
    "            anomaly_plots.append(fig)\n",
    "\n",
    "    # returns detected anomalies and generated plots for further processing\n",
    "    return anomalies, anomaly_plots\n",
    "\n",
    "def process_csv_files(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in the input folder and generates analysis results.\n",
    "\n",
    "    Params:\n",
    "    input_folder (str): Path to the folder containing CSV files.\n",
    "    output_folder (str): Path to the folder where results will be saved.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Summary of detected anomalies and downtimes across all files.\n",
    "    \"\"\"\n",
    "    # initializes lists to store anomalies and plots across all files for summary generation\n",
    "    all_anomalies = []\n",
    "    all_plots = []\n",
    "\n",
    "    # lists all CSV files in the input folder for batch processing\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    # processes each file one by one to extract and analyze battery data\n",
    "    for i, csv_file in enumerate(csv_files, 1):\n",
    "        # constructs the full path to the current file for loading\n",
    "        file_path = os.path.join(input_folder, csv_file)\n",
    "        # logs the progress of the file being processed for user feedback\n",
    "        print(f\"Processing file {i} of {len(csv_files)}: {csv_file}\")\n",
    "\n",
    "        try:\n",
    "            # loads and cleans the data from the specified file for analysis\n",
    "            df = load_and_clean_battery_data(file_path)\n",
    "            \n",
    "            # extracts the file name without extension for labeling plots\n",
    "            file_name = os.path.splitext(csv_file)[0]\n",
    "\n",
    "            # detects anomalies and generates plots using threshold analysis\n",
    "            anomalies, plots = plot_min_voltages_with_dynamic_thresholds(df, file_name, deviation_factor=0.025)\n",
    "            # aggregates anomalies and plots across all processed files\n",
    "            all_anomalies.extend(anomalies)\n",
    "            all_plots.extend(plots)\n",
    "\n",
    "            # logs the number of anomalies and plots generated for the current file\n",
    "            print(f\"Finished processing {csv_file}. {len(anomalies)} anomalies detected. {len(plots)} plots generated.\")\n",
    "        # handles errors during file processing and logs them for troubleshooting\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file}: {str(e)}\")\n",
    "\n",
    "    # creates a DataFrame from all detected anomalies for summary and analysis\n",
    "    anomaly_df = pd.DataFrame(all_anomalies)\n",
    "\n",
    "    # checks if any anomalies were detected; if not, prepares an empty DataFrame for output\n",
    "    if not anomaly_df.empty:\n",
    "        # groups anomalies by 'Rack' and 'Type' for summary statistics\n",
    "        summary = anomaly_df.groupby(['Rack', 'Type']).agg({\n",
    "            'Start_Time': 'min',  # identifies the first occurrence of an anomaly\n",
    "            'End_Time': 'max',  # identifies the last occurrence of an anomaly\n",
    "            'Duration': 'sum',  # calculates total duration of anomalies\n",
    "            'Max_RoC': lambda x: x.max() if x.notna().any() else None  # finds the maximum rate of change for significant anomalies\n",
    "        }).reset_index()\n",
    "\n",
    "        # renames columns for clarity in the output summary\n",
    "        summary.columns = ['Rack', 'Type', 'First anomaly time', 'Last anomaly time', 'Total duration', 'Max Rate of Change']\n",
    "\n",
    "        # calculates the number of detected anomalies for each 'Rack' and 'Type' combination\n",
    "        anomaly_counts = anomaly_df.groupby(['Rack', 'Type']).size().reset_index(name='Number of anomalies detected')\n",
    "        # merges anomaly counts with the summary DataFrame for a complete view\n",
    "        summary = summary.merge(anomaly_counts, on=['Rack', 'Type'])\n",
    "\n",
    "        # reorders columns for better readability in the output summary\n",
    "        summary = summary[['Rack', 'Type', 'Number of anomalies detected', 'First anomaly time', 'Last anomaly time', 'Total duration', 'Max Rate of Change']]\n",
    "\n",
    "        # sorts the summary by 'Rack' and 'Type' for consistent output ordering\n",
    "        summary = summary.sort_values(['Rack', 'Type'])\n",
    "    else:\n",
    "        # prepares an empty summary DataFrame with the required columns when no anomalies are detected\n",
    "        summary = pd.DataFrame(columns=['Rack', 'Type', 'Number of anomalies detected', 'First anomaly time', 'Last anomaly time', 'Total duration', 'Max Rate of Change'])\n",
    "    \n",
    "    # saves the summary DataFrame as a CSV file for external review and documentation\n",
    "    summary_csv_path = os.path.join(output_folder, 'combined_anomaly_summary.csv')\n",
    "    summary.to_csv(summary_csv_path, index=False)\n",
    "    print(f\"\\nCombined anomaly and downtime summary exported to: {summary_csv_path}\")\n",
    "\n",
    "    # checks if any plots were generated; if so, saves them as an interactive HTML file\n",
    "    if all_plots:\n",
    "        # defines the file path for saving all plots into a single HTML file\n",
    "        html_path = os.path.join(output_folder, 'anomaly_plots.html')\n",
    "        with open(html_path, 'w') as f:\n",
    "            # writes basic HTML structure to contain Plotly graphs\n",
    "            f.write('<html><head><title>Anomaly Plots</title></head><body>')\n",
    "            # iterates through each Plotly figure and adds it to the HTML file\n",
    "            for plot in all_plots:\n",
    "                f.write(plot.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "            f.write('</body></html>')\n",
    "        print(f\"Interactive plots saved to: {html_path}\")\n",
    "        print(f\"Total number of plots generated: {len(all_plots)}\")\n",
    "    else:\n",
    "        # logs that no plots were generated if no anomalies were detected across all files\n",
    "        print(\"No anomaly plots generated.\")\n",
    "    \n",
    "    # returns the final summary DataFrame for potential use in other parts of the program\n",
    "    return summary\n",
    "\n",
    "# main execution block ensures that the following code only runs when this script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    # sets paths for input folder containing CSV files and output folder for saving results\n",
    "\n",
    "    '''\n",
    "    input folder should only contain files that you wish to detect anomalies. No other CSV Files should be here\n",
    "    '''\n",
    "    input_folder = '/Users/lennox/Desktop/Lennox'\n",
    "    output_folder = '/Users/lennox/Desktop/Lennox/output'\n",
    "    \n",
    "    # checks if the output folder exists; creates it if not to ensure results can be saved\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # logs the start of the analysis process to provide user feedback\n",
    "    print(\"Starting analysis of all CSV files in the folder...\")\n",
    "    \n",
    "    # processes all files and generates a summary of detected anomalies and downtimes\n",
    "    anomaly_summary = process_csv_files(input_folder, output_folder)\n",
    "    \n",
    "    # displays the summary of detected anomalies and downtimes to the user for quick review\n",
    "    print(\"\\nAnalysis complete. Summary of detected anomalies and downtimes:\")\n",
    "    print(anomaly_summary)\n",
    "    \n",
    "    # informs the user that all plots have been saved and the analysis is complete\n",
    "    print(\"\\nAll plots have been generated and saved in the output folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78281a0d-aa93-461e-a8bb-45ef5e78d748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
